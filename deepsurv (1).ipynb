{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fINXWTiV1Fu"
      },
      "source": [
        "# DeepSurv\n",
        "\n",
        "In this notebook we will train the [Cox-PH method](http://jmlr.org/papers/volume20/18-424/18-424.pdf), also known as [DeepSurv](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0482-1)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtuples\n",
        "!pip install pycox"
      ],
      "metadata": {
        "id": "njmtOoQkWKrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx8CDAyAV1Fz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn_pandas import DataFrameMapper\n",
        "\n",
        "import torch\n",
        "import torchtuples as tt\n",
        "\n",
        "from pycox.datasets import metabric\n",
        "from pycox.models import CoxPH\n",
        "from pycox.evaluation import EvalSurv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSJIU3gTV1F1"
      },
      "outputs": [],
      "source": [
        "## Uncomment to install `sklearn-pandas`\n",
        "# ! pip install sklearn-pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMAOktxXV1F2"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1234)\n",
        "_ = torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXF8k8H7V1F2"
      },
      "source": [
        "## 1. Dataset\n",
        "\n",
        "We load the dataset and split in train, test and validation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install scikit-survival"
      ],
      "metadata": {
        "id": "tRt_yk3VhD8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sksurv.datasets import load_veterans_lung_cancer\n",
        "from sksurv.column import encode_categorical\n",
        "data_x, y = load_veterans_lung_cancer()\n",
        "x = data_x"
      ],
      "metadata": {
        "id": "YvZP0hDSgZWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are several categorical features that need to be encoded into one-hot vectors.\n",
        "# From category to numerical\n",
        "#category_columns = ['Celltype', 'Prior_therapy', 'Treatment']\n",
        "#x = pd.get_dummies(x, columns=category_columns, drop_first=True)"
      ],
      "metadata": {
        "id": "7lfVTCkBppdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_x.head()"
      ],
      "metadata": {
        "id": "H0yI0EUDiCXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "x['Celltype'] = label_encoder.fit_transform(x['Celltype'])\n",
        "\n",
        "x['Prior_therapy'] = LabelEncoder().fit_transform(x['Prior_therapy'])\n",
        "\n",
        "x['Treatment'] = LabelEncoder().fit_transform(x['Treatment'])"
      ],
      "metadata": {
        "id": "xGa5Ji3kibK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.head()"
      ],
      "metadata": {
        "id": "xLaBMW6ljCSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert y to DataFrame\n",
        "y_df = pd.DataFrame(y.tolist(), columns=['Status', 'Survival_in_days'])\n",
        "y_df['Status'] = y_df['Status'].astype(int)"
      ],
      "metadata": {
        "id": "KZPh06zNjfzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine x and y along columns\n",
        "combined_df = pd.concat([x, y_df], axis=1)"
      ],
      "metadata": {
        "id": "7XkaM0Z3i-bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combined_df.rename(columns={\n",
        "#          'Age_in_years': 'Age',\n",
        "#          'Celltype': 'CellType',\n",
        "#          'Karnofsky_score': 'KarnofskyScore',\n",
        "#          'Months_from_Diagnosis': 'MonthsFromDiagnosis',\n",
        "#          'Prior_therapy': 'PriorTherapy',\n",
        "#          'Treatment': 'Treatment',\n",
        "#          'Status': 'Status',\n",
        "#          'Survival_in_days': 'SurvivalDays'\n",
        "#}, inplace=True)"
      ],
      "metadata": {
        "id": "iD3IhQ4jlwBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "PRZ8I96fsP85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.info()"
      ],
      "metadata": {
        "id": "voylRGEwrbqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = np.setdiff1d(combined_df.columns, ['Status', 'Survival_in_days']).tolist()"
      ],
      "metadata": {
        "id": "b_cIhjiRrnFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for null values\n",
        "N_null = sum(combined_df[features].isnull().sum())\n",
        "print(\"The combined_df contains {} null values\".format(N_null)) #0 null values\n",
        "\n",
        "# Removing duplicates if there exist\n",
        "N_dupli = sum(combined_df.duplicated(keep='first'))\n",
        "combined_df = combined_df.drop_duplicates(keep='first').reset_index(drop=True)\n",
        "print(\"The dataset contains {} duplicates\".format(N_dupli))\n",
        "\n",
        "# Number of samples in the dataset\n",
        "N = combined_df.shape[0]"
      ],
      "metadata": {
        "id": "6JFHVknwredh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical features"
      ],
      "metadata": {
        "id": "C0GLhmlwseRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for feature in ['Age_in_years', 'Karnofsky_score', 'Months_from_Diagnosis']:\n",
        "\n",
        "    # Creating an empty chart\n",
        "    fig, ((ax1, ax2)) = plt.subplots(1, 2,  figsize=(15, 4))\n",
        "\n",
        "    # Extracting the feature values\n",
        "    x = combined_df[feature].values\n",
        "\n",
        "    # Boxplot\n",
        "    ax1.boxplot(x)\n",
        "    ax1.set_title( 'Boxplot for {}'.format(feature) )\n",
        "\n",
        "    # Histogram\n",
        "    ax2.hist(x, bins=20)\n",
        "    ax2.set_title( 'Histogram for {}'.format(feature) )\n",
        "\n",
        "    # Display\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2Ylan91zS4cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical features"
      ],
      "metadata": {
        "id": "d1JOZDQLUCG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "for feature in ['Celltype', 'Prior_therapy', 'Treatment']:\n",
        "\n",
        "    # Creating an empty chart\n",
        "    fig, ax = plt.subplots(figsize=(15, 4))\n",
        "\n",
        "    # Extracting the feature values\n",
        "    x = combined_df[feature].values\n",
        "\n",
        "    # Counting the number of occurrences for each category\n",
        "    data = Counter(x)\n",
        "    category = list(data.keys())\n",
        "    counts = list(data.values())\n",
        "\n",
        "    # Boxplot\n",
        "    ax.bar(category, counts)\n",
        "\n",
        "    # Display\n",
        "    plt.title( 'Barchart for {}'.format(feature) )\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pyOAqLPpULm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time & Event"
      ],
      "metadata": {
        "id": "iPhjERhTVe1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an empty chart\n",
        "fig, ((ax1, ax2)) = plt.subplots(1, 2,  figsize=(15, 4))\n",
        "\n",
        "# Counting the number of occurrences for each category\n",
        "data = Counter(combined_df['Status'].replace({0:'censored', 1:'death'}))\n",
        "category = list(data.keys())\n",
        "counts = list(data.values())\n",
        "idx = range(len(counts))\n",
        "\n",
        "# Displaying the occurrences of the event/censoring\n",
        "ax1.bar(idx, counts)\n",
        "ax1.set_xticks(idx)\n",
        "ax1.set_xticklabels(category)\n",
        "ax1.set_title( 'Occurences of the event/censoring', fontsize=15)\n",
        "\n",
        "# Showing the histogram of the survival times for the censoring\n",
        "time_0 = combined_df.loc[ combined_df['Status'] == 0, 'Survival_in_days']\n",
        "ax2.hist(time_0, bins=30, alpha=0.3, color='blue', label = 'censored')\n",
        "\n",
        "# Showing the histogram of the survival times for the events\n",
        "time_1 = combined_df.loc[ combined_df['Status'] == 1, 'Survival_in_days']\n",
        "ax2.hist(time_1, bins=20, alpha=0.7, color='black', label = 'death')\n",
        "ax2.set_title( 'Histogram - survival time', fontsize=15)\n",
        "\n",
        "# Displaying everything side-by-side\n",
        "plt.legend(fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iIIXag0cVcUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_censored = y.shape[0] - y[\"Status\"].sum()\n",
        "print(\"%.1f%% of records are censored\" % (n_censored / y.shape[0] * 100))"
      ],
      "metadata": {
        "id": "wdktXpmAhd8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9, 6))\n",
        "val, bins, patches = plt.hist((y[\"Survival_in_days\"][y[\"Status\"]],\n",
        "                               y[\"Survival_in_days\"][~y[\"Status\"]]),\n",
        "                              bins=30, stacked=True)\n",
        "plt.legend(patches, [\"Time of Death\", \"Time of Censoring\"])"
      ],
      "metadata": {
        "id": "lbO02IDpiBl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlations"
      ],
      "metadata": {
        "id": "u2tQq9TSXPjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "corr = combined_df[features].corr()\n",
        "\n",
        "sns.heatmap(corr,annot=True)"
      ],
      "metadata": {
        "id": "O_0YfJbzX6no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Modelling"
      ],
      "metadata": {
        "id": "DxEE0V36Zihb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train, valid, and test set split"
      ],
      "metadata": {
        "id": "ulZOTmbbr-u-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAHusj9WV1F3"
      },
      "outputs": [],
      "source": [
        "# metabric.read_df()\n",
        "df_train = combined_df\n",
        "df_test = df_train.sample(frac=0.2)\n",
        "df_train = df_train.drop(df_test.index)\n",
        "\n",
        "df_val = df_train.sample(frac=0.2)\n",
        "df_train = df_train.drop(df_val.index)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "CicP6KoMkrxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbI2BpMVV1F3"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFjDmtT_V1F4"
      },
      "source": [
        "### Feature transforms\n",
        "We have 9 covariates, in addition to the durations and event indicators.\n",
        "\n",
        "We will standardize the 5 numerical covariates, and leave the binary variables as is. As variables needs to be of type `'float32'`, as this is required by pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3qeXdyKV1F5"
      },
      "outputs": [],
      "source": [
        "cols_standardize = ['Age_in_years', 'Karnofsky_score', 'Months_from_Diagnosis']\n",
        "cols_leave = ['Celltype', 'Prior_therapy', 'Treatment']\n",
        "\n",
        "standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
        "leave = [(col, None) for col in cols_leave]\n",
        "\n",
        "x_mapper = DataFrameMapper(standardize + leave)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IIvL-8tV1F5"
      },
      "outputs": [],
      "source": [
        "x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
        "x_val = x_mapper.transform(df_val).astype('float32')\n",
        "x_test = x_mapper.transform(df_test).astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GljwA3l4V1F6"
      },
      "source": [
        "We need no label transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DidNkMLbV1F6"
      },
      "outputs": [],
      "source": [
        "get_target = lambda df: (df['Survival_in_days'].values, df['Status'].values)\n",
        "y_train = get_target(df_train)\n",
        "y_val = get_target(df_val)\n",
        "durations_test, events_test = get_target(df_test)\n",
        "val = x_val, y_val"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = df['Survival_in_days'].values+ df['Status'].values"
      ],
      "metadata": {
        "id": "Uv8-HHamdUXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoUzG-6iV1F7"
      },
      "source": [
        "### Neural net\n",
        "\n",
        "We create a simple MLP with two hidden layers, ReLU activations, batch norm and dropout.\n",
        "Here, we just use the `torchtuples.practical.MLPVanilla` net to do this.\n",
        "\n",
        "Note that we set `out_features` to 1, and that we have not `output_bias`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vsYzkspV1F7"
      },
      "outputs": [],
      "source": [
        "in_features = x_train.shape[1]\n",
        "num_nodes = [32, 32]\n",
        "out_features = 1\n",
        "batch_norm = True\n",
        "dropout = 0.1\n",
        "output_bias = False\n",
        "\n",
        "net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm,\n",
        "                              dropout, output_bias=output_bias)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a wrapper\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "import torchtuples as tt\n",
        "\n",
        "\n",
        "class DeepSURVSklearnAdapter(BaseEstimator):\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate=1e-4,\n",
        "        batch_norm=True,\n",
        "        dropout=0.0,\n",
        "        num_nodes=[32, 32],\n",
        "        batch_size=128,\n",
        "        epochs=10,\n",
        "    ):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_norm = batch_norm\n",
        "        self.dropout = dropout\n",
        "        self.num_nodes = num_nodes\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.net_ = tt.practical.MLPVanilla(\n",
        "            X.shape[1],\n",
        "            self.num_nodes,\n",
        "            1,\n",
        "            self.batch_norm,\n",
        "            self.dropout,\n",
        "            output_bias=True,\n",
        "        )\n",
        "        self.deepsurv_ = CoxPH(self.net_, tt.optim.Adam)\n",
        "        self.deepsurv_.optimizer.set_lr(self.learning_rate)\n",
        "\n",
        "        # Sklearn needs the y inputs to be arranged as a matrix with each row\n",
        "        # corresponding to an example but CoxPH needs a tuple with two arrays?\n",
        "        y_ = (y[:, 0], y[:, 1])\n",
        "\n",
        "        callbacks = [tt.callbacks.EarlyStopping()]\n",
        "        log = self.deepsurv_.fit(\n",
        "            X,\n",
        "            y_,\n",
        "            self.batch_size,\n",
        "            self.epochs,\n",
        "            verbose=False,\n",
        "        )\n",
        "\n",
        "        return self\n",
        "\n",
        "    def score(self, X, y):\n",
        "        _ = self.deepsurv_.compute_baseline_hazards()\n",
        "        surv = self.deepsurv_.predict_surv_df(X)\n",
        "\n",
        "        ev = EvalSurv(\n",
        "            surv,\n",
        "            y[:, 0],  # time to event\n",
        "            y[:, 1],  # event\n",
        "            censor_surv=\"km\",\n",
        "        )\n",
        "\n",
        "        return ev.concordance_td()"
      ],
      "metadata": {
        "id": "K22V0jkPgSWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterSampler\n",
        "\n",
        "# Define the parameter grid for random search\n",
        "param_grid = {\n",
        "    'learning_rate': [1e-2, 1e-3, 1e-4],\n",
        "    'dropout': [0.0, 0.1, 0.2],\n",
        "    'num_nodes': [[32, 32], [64, 64], [32, 64]],\n",
        "    'batch_size': [128, 256, 512],\n",
        "    'epochs': [50, 100, 200]\n",
        "}\n",
        "\n",
        "# Define the number of parameter settings that will be sampled\n",
        "n_iter = 10\n",
        "\n",
        "# Perform random search\n",
        "random_search = ParameterSampler(param_grid, n_iter=n_iter, random_state=123)\n",
        "\n",
        "best_score = float('-inf')\n",
        "best_params = None\n",
        "\n",
        "for params in random_search:\n",
        "    print(\"Current parameters:\", params)\n",
        "    model = DeepSURVSklearnAdapter(**params)\n",
        "    model.fit(x_train, y_train)\n",
        "    c_index = model.score(x_val, y_val)\n",
        "    print(\"C-index:\", c_index)\n",
        "    if c_index > best_score:\n",
        "        best_score = c_index\n",
        "        best_params = params\n",
        "\n",
        "print(\"Best parameters:\", best_params)\n",
        "print(\"Best C-index:\", best_score)"
      ],
      "metadata": {
        "id": "eRMc2dOfnKN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBHTuT9PV1F7"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "To train the model we need to define an optimizer. You can choose any `torch.optim` optimizer, but here we instead use one from `tt.optim` as it has some added functionality.\n",
        "We use the `Adam` optimizer, but instead of choosing a learning rate, we will use the scheme proposed by [Smith 2017](https://arxiv.org/pdf/1506.01186.pdf) to find a suitable learning rate with `model.lr_finder`. See [this post](https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6) for an explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLSkW96AV1F7"
      },
      "outputs": [],
      "source": [
        "model = CoxPH(net, tt.optim.Adam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSL-GHp9V1F8"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "lrfinder = model.lr_finder(x_train, y_train, batch_size, tolerance=10)\n",
        "_ = lrfinder.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIBxjJZZV1F8"
      },
      "outputs": [],
      "source": [
        "print(\"Best learning rate:\")\n",
        "lrfinder.get_best_lr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pEStHOBV1F8"
      },
      "source": [
        "Often, this learning rate is a little high, so we instead set it manually to 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmYZWHwtV1F8"
      },
      "outputs": [],
      "source": [
        "model.optimizer.set_lr(0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlbpFef3V1F8"
      },
      "source": [
        "We include the `EarlyStopping` callback to stop training when the validation loss stops improving. After training, this callback will also load the best performing model in terms of validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weEYNA8PV1F9"
      },
      "outputs": [],
      "source": [
        "epochs = 512\n",
        "callbacks = [tt.callbacks.EarlyStopping()]\n",
        "verbose = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae8r9JY5V1F9"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "history = model.fit(x_train, y_train, batch_size, epochs, callbacks, verbose,\n",
        "                val_data=val, val_batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m64p42Q5V1F9"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation loss over epochs\n",
        "_ = history.plot()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4CbfJcQOelCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLAjvdGIV1F9"
      },
      "source": [
        "We can get the partial log-likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1EQGFjjV1F9"
      },
      "outputs": [],
      "source": [
        "model.partial_log_likelihood(*val).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean value of the concordance index (C-Index) across 5 repeats of 5-fold cross-validation (可能不需要?)"
      ],
      "metadata": {
        "id": "yyqoPbQqC3Ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Prepare data for pycox\n",
        "# Example using metabric dataset from pycox\n",
        "# You need to adapt this part based on your dataset\n",
        "df = metabric.read_df()\n",
        "df.dropna(inplace=True)\n",
        "df = df[['duration', 'event'] + [col for col in df.columns if col.startswith('x')]]\n",
        "\n",
        "# Convert pandas DataFrame to PyTorch tensors\n",
        "X = torch.FloatTensor(df.drop(['duration', 'event'], axis=1).values)\n",
        "T = df['duration'].values  # Use NumPy array instead of torch.FloatTensor\n",
        "E = df['event'].values  # Use NumPy array instead of torch.FloatTensor\n",
        "\n",
        "c_index_values = []  # To store C-Index values for each fold and repeat\n",
        "\n",
        "# Set up 5 repeats of 5-fold cross-validation\n",
        "n_repeats = 5\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits)\n",
        "\n",
        "for i_repeat in range(n_repeats):\n",
        "    print(f\"Repeat {i_repeat + 1}/{n_repeats}\")\n",
        "\n",
        "    for i_fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
        "        print(f\"\\tFold {i_fold + 1}/{n_splits}\")\n",
        "\n",
        "        # Split data into training and testing sets\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        T_train, T_test = T[train_index], T[test_index]\n",
        "        E_train, E_test = E[train_index], E[test_index]\n",
        "\n",
        "        # Define and train your deep survival model (DeepHit, for example)\n",
        "        # Adjust parameters based on your model architecture and settings\n",
        "        model = CoxPH(net, tt.optim.Adam)\n",
        "\n",
        "        model.fit(x_train, y_train, batch_size, epochs, callbacks, verbose,\n",
        "                val_data=val, val_batch_size=batch_size)\n",
        "\n",
        "        # Compute baseline hazards\n",
        "        model.compute_baseline_hazards()\n",
        "\n",
        "        # Evaluate the model on the testing data\n",
        "        surv = model.predict_surv_df(X_test)\n",
        "        ev = EvalSurv(surv, T_test, E_test)  # Remove 'censor_surv' argument\n",
        "        # Compute evaluation metrics (e.g., concordance index)\n",
        "        c_index = ev.concordance_td()\n",
        "        c_index_values.append(c_index)  # Store C-Index for this fold and repeat\n",
        "        print(f\"\\tC-index: {c_index}\")"
      ],
      "metadata": {
        "id": "8duiYPLhBl0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean C-Index across all folds and repeats\n",
        "mean_c_index = np.mean(c_index_values)\n",
        "print(f\"Mean C-index across all folds and repeats: {mean_c_index}\")"
      ],
      "metadata": {
        "id": "u_0vcNDRDGg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ijPvCuV1F-"
      },
      "source": [
        "## Prediction\n",
        "\n",
        "For evaluation we first need to obtain survival estimates for the test set.\n",
        "This can be done with `model.predict_surv` which returns an array of survival estimates, or with `model.predict_surv_df` which returns the survival estimates as a dataframe.\n",
        "\n",
        "However, as `CoxPH` is semi-parametric, we first need to get the non-parametric baseline hazard estimates with `compute_baseline_hazards`.\n",
        "\n",
        "Note that for large datasets the `sample` argument can be used to estimate the baseline hazard on a subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YryyKNheV1F-"
      },
      "outputs": [],
      "source": [
        "_ = model.compute_baseline_hazards()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm0PNk1UV1F-"
      },
      "outputs": [],
      "source": [
        "# surv: predicted survival probability\n",
        "surv = model.predict_surv_df(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEwOxl7UV1F-"
      },
      "outputs": [],
      "source": [
        "surv.iloc[:, :5].plot()\n",
        "plt.ylabel('S(t | x)')\n",
        "_ = plt.xlabel('Time')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the hazard function\n",
        "hazard_function = -np.log(surv).diff(axis=1)\n",
        "\n",
        "# The first column of the hazard function contain NaN value\n",
        "# replace them with 0\n",
        "hazard_function[hazard_function.columns[0]] = 0"
      ],
      "metadata": {
        "id": "0DKIzP4428OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot hazard function\n",
        "hazard_function.iloc[:, :5].plot(style='--', title='Hazard Functions for the First 5 Individuals')\n",
        "plt.ylabel('Hazard Function')\n",
        "plt.xlabel('Time')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eyUOra1D3NJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li3LjOIQV1F-"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We can use the `EvalSurv` class for evaluation the concordance, brier score and binomial log-likelihood. Setting `censor_surv='km'` means that we estimate the censoring distribution by Kaplan-Meier on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_P29QxCV1F_"
      },
      "outputs": [],
      "source": [
        "ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9DZBLcYV1F_"
      },
      "outputs": [],
      "source": [
        "# Calculate the C-index\n",
        "c_index = ev.concordance_td()\n",
        "print(\"C-index:\", c_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxA9yOgHV1GA"
      },
      "outputs": [],
      "source": [
        "# Create a time grid with 100 equally spaced time points\n",
        "time_grid = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
        "_ = ev.brier_score(time_grid).plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C-sBr8cV1GA"
      },
      "outputs": [],
      "source": [
        "ev.integrated_brier_score(time_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocyK1A1VV1GA"
      },
      "outputs": [],
      "source": [
        "ev.integrated_nbll(time_grid)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}