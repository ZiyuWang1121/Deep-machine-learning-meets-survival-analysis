{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZiyuWang1121/Deep-machine-learning-meets-survival-analysis/blob/main/DeepSurv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fINXWTiV1Fu"
      },
      "source": [
        "# DeepSurv\n",
        "\n",
        "In this notebook we will train the [Cox-PH method](http://jmlr.org/papers/volume20/18-424/18-424.pdf), also known as [DeepSurv](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0482-1)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtuples\n",
        "!pip install pycox"
      ],
      "metadata": {
        "id": "njmtOoQkWKrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx8CDAyAV1Fz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn_pandas import DataFrameMapper\n",
        "\n",
        "import torch\n",
        "import torchtuples as tt\n",
        "\n",
        "from pycox.models import CoxPH\n",
        "from pycox.evaluation import EvalSurv\n",
        "\n",
        "from sklearn.model_selection import ParameterSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMAOktxXV1F2"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1234)\n",
        "_ = torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXF8k8H7V1F2"
      },
      "source": [
        "## 1. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install scikit-survival"
      ],
      "metadata": {
        "id": "tRt_yk3VhD8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YvZP0hDSgZWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the preprocessed file\n",
        "brca = pd.read_csv('/content/drive/My Drive/3799/brca.csv')"
      ],
      "metadata": {
        "id": "F3jyV91MV0og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# Apply PCA to reduce dimensionality\n",
        "pca = PCA(n_components=0.99)  # Retain 95% of the variance\n",
        "x_pca = pca.fit_transform(brca.iloc[:,2:])\n",
        "x_pca.shape"
      ],
      "metadata": {
        "id": "28dJtkCN2tYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize dimension reduction results\n",
        "plt.figure(figsize=(12, 6))\n",
        "# component versus survival time\n",
        "plt.scatter(x_pca[:,0], x_pca[:,1],c=brca.iloc[:,2], cmap='viridis', marker='o')\n",
        "plt.colorbar()\n",
        "plt.title('PCA visualization of the data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gISbelJ-MqQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating the dataframes\n",
        "brca_pca = pd.concat([brca.iloc[:, :2], pd.DataFrame(x_pca)], axis=1)"
      ],
      "metadata": {
        "id": "9LafL9gM3V5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###t-SNE"
      ],
      "metadata": {
        "id": "3G7Zw9dSRHWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Nonlinear dimensionality reduction with t-SNE\n",
        "#tsne = TSNE(n_components=3, random_state=0)\n",
        "#x_tsne = tsne.fit_transform(brca.iloc[:,2:])"
      ],
      "metadata": {
        "id": "-i0xk41GEDVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize dimension reduction results\n",
        "#plt.figure(figsize=(12, 6))\n",
        "# component versus survival time\n",
        "#plt.scatter(x_tsne[:,0], x_tsne[:,1],c=brca.iloc[:,2], cmap='viridis', marker='o')\n",
        "#plt.colorbar()\n",
        "#plt.title('t-SNE visualization of the data')\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "WvWzir_cI9JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x_tsne.shape"
      ],
      "metadata": {
        "id": "tuk6r5LMEOI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating the dataframes\n",
        "#brca_tsne = pd.concat([brca.iloc[:, :2], pd.DataFrame(x_tsne)], axis=1)"
      ],
      "metadata": {
        "id": "UHsU_sIgER7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Modelling"
      ],
      "metadata": {
        "id": "DxEE0V36Zihb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train, valid, and test set split"
      ],
      "metadata": {
        "id": "ulZOTmbbr-u-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = brca_pca"
      ],
      "metadata": {
        "id": "2uwqA0M4N5XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_train = data\n",
        "#df_test = df_train.sample(frac=0.2)\n",
        "#df_train = df_train.drop(df_test.index)\n",
        "\n",
        "#df_val = df_train.sample(frac=0.2)\n",
        "#df_train = df_train.drop(df_val.index)"
      ],
      "metadata": {
        "id": "yZXoolx_iQMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the dataset into train, validation, and test sets while maintaining class balance\n",
        "df_train_val, df_test = train_test_split(data, test_size=0.2, random_state=42, stratify=data['status'])\n",
        "df_train, df_val = train_test_split(df_train_val, test_size=0.2, random_state=42, stratify=df_train_val['status'])\n",
        "\n",
        "#df_train_val, df_test = train_test_split(data, test_size=0.2, random_state=42, stratify=data['status'])\n",
        "#df_train, df_val = train_test_split(df_train_val, test_size=0.2, random_state=42, stratify=df_train_val['status'])"
      ],
      "metadata": {
        "id": "BsdwfKlpI2Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFjDmtT_V1F4"
      },
      "source": [
        "### Feature transforms\n",
        "All variables needs to be of type `'float32'`, as this is required by pytorch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn_pandas import DataFrameMapper\n",
        "# Get a list of all column names in the DataFrame\n",
        "all_cols = data.iloc[:,2:].columns.tolist()\n",
        "\n",
        "# Create a list of tuples for all columns\n",
        "all_cols_tuples = [(col, None) for col in all_cols]\n",
        "\n",
        "# Create the DataFrameMapper object with all columns\n",
        "x_mapper = DataFrameMapper(all_cols_tuples)"
      ],
      "metadata": {
        "id": "mI-hwKMJrNPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_mapper.fit_transform(df_train).astype('float32')\n",
        "x_val = x_mapper.transform(df_val).astype('float32')\n",
        "x_test = x_mapper.transform(df_test).astype('float32')"
      ],
      "metadata": {
        "id": "4AtyMy1Lth-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DidNkMLbV1F6"
      },
      "outputs": [],
      "source": [
        "# Extract target\n",
        "get_target = lambda df: (df['time'].values, df['status'].values)\n",
        "y_train = get_target(df_train)\n",
        "y_val = get_target(df_val)\n",
        "y_test=get_target(df_test)\n",
        "durations_test, events_test = get_target(df_test)\n",
        "val = x_val, y_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoUzG-6iV1F7"
      },
      "source": [
        "### Neural net\n",
        "\n",
        "We create a simple MLP with two hidden layers, ReLU activations, batch norm and dropout.\n",
        "Here, we just use the `torchtuples.practical.MLPVanilla` net to do this.\n",
        "\n",
        "Note that we set `out_features` to 1, and that we have not `output_bias`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class DeepSURVSklearnAdapter(BaseEstimator):\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate=1e-4,\n",
        "        batch_norm=True,\n",
        "        dropout=0.0,\n",
        "        num_nodes=[32, 32],\n",
        "        batch_size=128,\n",
        "        epochs=10,\n",
        "    ):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_norm = batch_norm\n",
        "        self.dropout = dropout\n",
        "        self.num_nodes = num_nodes\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.net_ = tt.practical.MLPVanilla(\n",
        "            X.shape[1],\n",
        "            self.num_nodes,\n",
        "            1,\n",
        "            self.batch_norm,\n",
        "            self.dropout,\n",
        "            output_bias = False, # prevent overfitting/simplify the model\n",
        "        )\n",
        "        self.deepsurv_ = CoxPH(self.net_, tt.optim.Adam)\n",
        "        self.deepsurv_.optimizer.set_lr(self.learning_rate)\n",
        "\n",
        "        # Sklearn needs the y inputs to be arranged as a matrix with each row\n",
        "        # corresponding to an example but CoxPH needs a tuple with two arrays?\n",
        "        y_ = (y[0], y[1])\n",
        "\n",
        "        callbacks = [tt.callbacks.EarlyStopping()]\n",
        "        log = self.deepsurv_.fit(\n",
        "            X,\n",
        "            y_,\n",
        "            self.batch_size,\n",
        "            self.epochs,\n",
        "            verbose=False,\n",
        "        )\n",
        "\n",
        "        return self\n",
        "\n",
        "    def score(self, X, y):\n",
        "        _ = self.deepsurv_.compute_baseline_hazards()\n",
        "        surv = self.deepsurv_.predict_surv_df(X)\n",
        "\n",
        "        ev = EvalSurv(\n",
        "            surv,\n",
        "            y[0],  # time to event\n",
        "            y[1],  # event\n",
        "            censor_surv=\"km\",\n",
        "        )\n",
        "\n",
        "        return ev.concordance_td()\n",
        "\n",
        "    def brier_score(self, X, y, time_grid):\n",
        "        _ = self.deepsurv_.compute_baseline_hazards()\n",
        "        surv = self.deepsurv_.predict_surv_df(X)\n",
        "\n",
        "        ev = EvalSurv(\n",
        "            surv,\n",
        "            y[0],  # time to event\n",
        "            y[1],  # event\n",
        "            censor_surv=\"km\",\n",
        "        )\n",
        "\n",
        "        return ev.brier_score(time_grid)\n",
        "\n",
        "    def integrated_brier_score(self, X, y, time_grid):\n",
        "        _ = self.deepsurv_.compute_baseline_hazards()\n",
        "        surv = self.deepsurv_.predict_surv_df(X)\n",
        "\n",
        "        ev = EvalSurv(\n",
        "            surv,\n",
        "            y[0],  # time to event\n",
        "            y[1],  # event\n",
        "            censor_surv=\"km\",\n",
        "        )\n",
        "\n",
        "        return ev.integrated_brier_score(time_grid)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predict survival probabilities for X\n",
        "        return self.deepsurv_.predict_surv_df(X)"
      ],
      "metadata": {
        "id": "K22V0jkPgSWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for random search\n",
        "param_grid = {\n",
        "    'learning_rate': [1e-1, 1e-2, 1e-3, 1e-4],\n",
        "    'dropout': [0.0, 0.1, 0.2],\n",
        "    'num_nodes': [[32, 32], [64, 64], [32, 64]],\n",
        "    'batch_size': [128, 256, 512],\n",
        "    'epochs': [512]\n",
        "}\n",
        "\n",
        "# Define the number of parameter settings that will be sampled\n",
        "n_iter = 10\n",
        "\n",
        "# Perform random search\n",
        "random_search = ParameterSampler(param_grid, n_iter=n_iter, random_state=123)\n",
        "\n",
        "best_score = float('-inf')\n",
        "best_params = None\n",
        "\n",
        "for params in random_search:\n",
        "    print(\"Current parameters:\", params)\n",
        "    model = DeepSURVSklearnAdapter(**params)\n",
        "    model.fit(x_train, y_train)\n",
        "    c_index = model.score(x_val, y_val)\n",
        "    print(\"C-index:\", round(c_index,3))\n",
        "    if c_index > best_score:\n",
        "        best_score = c_index\n",
        "        best_params = params\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "print(\"Best parameters:\", best_params)\n",
        "print(\"Best C-index:\", best_score)"
      ],
      "metadata": {
        "id": "eRMc2dOfnKN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and Prediction"
      ],
      "metadata": {
        "id": "UKPWHwjNzhyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the best model with the best parameters\n",
        "best_model = DeepSURVSklearnAdapter(**best_params)\n",
        "best_model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "9-SmJYistifZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_c_index = best_model.score(x_train, y_train)\n",
        "print(\"C-index on train data:\", round(train_c_index,3))"
      ],
      "metadata": {
        "id": "FHg2hXWd2Kz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the best model on the test data\n",
        "test_c_index = best_model.score(x_test, y_test)\n",
        "print(\"C-index on test data:\", round(test_c_index,3))"
      ],
      "metadata": {
        "id": "lLYxUZ0z110n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using the best model\n",
        "predictions = best_model.predict(x_test)"
      ],
      "metadata": {
        "id": "UsnLb3vm1tcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction: predicted survival probability\n",
        "predictions.iloc[:, :5].plot()\n",
        "plt.ylabel('S(t | x)')\n",
        "_ = plt.xlabel('Time')"
      ],
      "metadata": {
        "id": "udIWg427wTRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the hazard function\n",
        "hazard_function = -np.log(predictions).diff(axis=1)"
      ],
      "metadata": {
        "id": "kcs8NFAowoE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot hazard function\n",
        "hazard_function.iloc[:, :5].plot(style='--', title='Hazard Functions for the First 5 Patients')\n",
        "plt.ylabel('Hazard Function')\n",
        "plt.xlabel('Time')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oKziQ1lxzBpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_grid = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
        "_ = best_model.brier_score(x_test, y_test, time_grid).plot()"
      ],
      "metadata": {
        "id": "UUHOv3-KVu5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Integrated brier score:\",best_model.integrated_brier_score(x_test, y_test, time_grid))"
      ],
      "metadata": {
        "id": "isQVJ6uzWoUo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}